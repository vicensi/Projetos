{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vicensi/Projetos/blob/main/Spark_Streaming_Twitter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjXAvjDMoSnr"
      },
      "source": [
        "### *********** Atenção: *********** Utilize Java JDK 1.8 e Apache Spark 2.2.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BneDfuKvoSnt"
      },
      "source": [
        "****** Caso receba mensagem de erro \"name 'sc' is not defined\", interrompa o pyspark e apague o diretório metastore_db no mesmo diretório onde está este Jupyter notebook ******"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fg6kzdeoSnt"
      },
      "source": [
        "Acesse http://localhost:4040 sempre que quiser acompanhar a execução dos jobs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBX89QK7oSnt"
      },
      "source": [
        "## Spark Streaming - Twitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwSx0ZlmoSnu"
      },
      "outputs": [],
      "source": [
        "# Pode ser necessário instalar esses pacotes\n",
        "# !pip install requests_oauthlib\n",
        "# !pip install twython\n",
        "# !pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0N5CWWWoSnu"
      },
      "outputs": [],
      "source": [
        "# Módulos usados\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark import SparkContext\n",
        "from requests_oauthlib import OAuth1Session\n",
        "from operator import add\n",
        "import requests_oauthlib\n",
        "from time import gmtime, strftime\n",
        "import requests\n",
        "import time\n",
        "import string\n",
        "import ast\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFRwgytGoSnv"
      },
      "outputs": [],
      "source": [
        "# Pacote NLTK\n",
        "import nltk\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.corpus import subjectivity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.util import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UILhJS5oSnv"
      },
      "outputs": [],
      "source": [
        "# Frequência de update\n",
        "INTERVALO_BATCH = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM1zkH2foSnv"
      },
      "outputs": [],
      "source": [
        "# Criando o StreamingContext\n",
        "ssc = StreamingContext(sc, INTERVALO_BATCH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPb1qXuYoSnv"
      },
      "source": [
        "## Treinando o Classificador de Análise de Sentimento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCJeN7BVoSnw"
      },
      "source": [
        "Uma parte essencial da criação de um algoritmo de análise de sentimento (ou qualquer algoritmo de mineração de dados) é ter um conjunto de dados abrangente ou \"Corpus\" para o aprendizado, bem como um conjunto de dados de teste para garantir que a precisão do seu algoritmo atende aos padrões que você espera. Isso também permitirá que você ajuste o seu algoritmo a fim de deduzir melhores (ou mais precisas) características de linguagem natural que você poderia extrair do texto e que vão contribuir para a classificação de sentimento, em vez de usar uma abordagem genérica. Tomaremos como base o dataset de treino fornecido pela Universidade de Michigan, para competições do Kaggle --> https://inclass.kaggle.com/c/si650winter11."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZiKl0hBoSnw"
      },
      "source": [
        "Esse dataset contém 1,578,627 tweets classificados e cada linha é marcada como:\n",
        "\n",
        "### 1 para o sentimento positivo\n",
        "### 0 para o sentimento negativo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1GpDbfroSnw"
      },
      "outputs": [],
      "source": [
        "# Lendo o arquivo texto e criando um RDD em memória com Spark\n",
        "#arquivo = sc.textFile(\"/opt/DSA/BigDataAnalytics-Python-Spark/Capitulo08/dataset_analise_sentimento.csv\")\n",
        "arquivo = sc.textFile(\"G:/GitHub/scripts_dsa/BigDataAnalytics-Python-Spark/Projetos/Projeto2/dataset_analise_sentimento.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYz8hHu0oSnw"
      },
      "outputs": [],
      "source": [
        "# Removendo o cabeçalho\n",
        "header = arquivo.take(1)[0]\n",
        "dataset = arquivo.filter(lambda line: line != header)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f_9qv2ZoSnw",
        "outputId": "12b0d163-c6ee-455e-81d1-39b2da9fa4da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pyspark.rdd.PipelinedRDD"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp_DEzlqoSnx"
      },
      "outputs": [],
      "source": [
        "# Essa função separa as colunas em cada linha, cria uma tupla e remove a pontuação.\n",
        "def get_row(line):\n",
        "  row = line.split(',')\n",
        "  sentimento = row[1]\n",
        "  tweet = row[3].strip()\n",
        "  translator = str.maketrans({key: None for key in string.punctuation})\n",
        "  tweet = tweet.translate(translator)\n",
        "  tweet = tweet.split(' ')\n",
        "  tweet_lower = []\n",
        "  for word in tweet:\n",
        "    tweet_lower.append(word.lower())\n",
        "  return (tweet_lower, sentimento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtCcmp6GoSnx"
      },
      "outputs": [],
      "source": [
        "# Aplica a função a cada linha do dataset\n",
        "dataset_treino = dataset.map(lambda line: get_row(line))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTWZz04coSnx"
      },
      "outputs": [],
      "source": [
        "# Cria um objeto SentimentAnalyzer\n",
        "sentiment_analyzer = SentimentAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfkZUN6XoSnx"
      },
      "outputs": [],
      "source": [
        "# Certifique-se de ter espaço em disco - Aproximadamente 5GB\n",
        "# https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
        "# nltk.download()\n",
        "# nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hc0wEsRYoSny",
        "outputId": "8fc09fcf-0ca1-4ff5-9c36-cfdc1eec9e28"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"ntlkdata.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "Image(url = 'ntlkdata.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qiMmqt_oSny"
      },
      "outputs": [],
      "source": [
        "# Obtém a lista de stopwords em Inglês\n",
        "stopwords_all = []\n",
        "for word in stopwords.words('english'):\n",
        "  stopwords_all.append(word)\n",
        "  stopwords_all.append(word + '_NEG')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH7fmL2_oSny"
      },
      "outputs": [],
      "source": [
        "# Obtém 10.000 tweets do dataset de treino e retorna todas as palavras que não são stopwords\n",
        "dataset_treino_amostra = dataset_treino.take(10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0_vDDRWoSny"
      },
      "outputs": [],
      "source": [
        "all_words_neg = sentiment_analyzer.all_words([mark_negation(doc) for doc in dataset_treino_amostra])\n",
        "all_words_neg_nostops = [x for x in all_words_neg if x not in stopwords_all]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnHH2eosoSny"
      },
      "outputs": [],
      "source": [
        "# Cria um unigram e extrai as features\n",
        "unigram_feats = sentiment_analyzer.unigram_word_feats(all_words_neg_nostops, top_n = 200)\n",
        "sentiment_analyzer.add_feat_extractor(extract_unigram_feats, unigrams = unigram_feats)\n",
        "training_set = sentiment_analyzer.apply_features(dataset_treino_amostra)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJVBlQPZoSny",
        "outputId": "e9e3b992-3b04-4078-b446-360456e60b12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "nltk.collections.LazyMap"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(training_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmgqWnIaoSny",
        "outputId": "35e8a245-e8ea-4606-96ef-6971c836ce64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training classifier\n"
          ]
        }
      ],
      "source": [
        "# Treinar o modelo\n",
        "trainer = NaiveBayesClassifier.train\n",
        "classifier = sentiment_analyzer.train(trainer, training_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7igI9LrGoSnz"
      },
      "outputs": [],
      "source": [
        "# Testa o classificador em algumas sentenças\n",
        "test_sentence1 = [(['this', 'program', 'is', 'bad'], '')]\n",
        "test_sentence2 = [(['tough', 'day', 'at', 'work', 'today'], '')]\n",
        "test_sentence3 = [(['good', 'wonderful', 'amazing', 'awesome'], '')]\n",
        "test_set = sentiment_analyzer.apply_features(test_sentence1)\n",
        "test_set2 = sentiment_analyzer.apply_features(test_sentence2)\n",
        "test_set3 = sentiment_analyzer.apply_features(test_sentence3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYcYcwYQoSnz"
      },
      "outputs": [],
      "source": [
        "# Autenticação do Twitter\n",
        "consumer_key = \"\"\n",
        "consumer_secret = \"\"\n",
        "access_token = \"\"\n",
        "access_token_secret = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HMStCTGoSnz"
      },
      "outputs": [],
      "source": [
        "# Especifica a URL termo de busca\n",
        "search_term = 'Trump'\n",
        "sample_url = 'https://stream.twitter.com/1.1/statuses/sample.json'\n",
        "filter_url = 'https://stream.twitter.com/1.1/statuses/filter.json?track='+search_term"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEaTsaiVoSnz"
      },
      "outputs": [],
      "source": [
        "# Criando o objeto de atutenticação para o Twitter\n",
        "auth = requests_oauthlib.OAuth1(consumer_key, consumer_secret, access_token, access_token_secret)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxmofYw1oSnz"
      },
      "outputs": [],
      "source": [
        "# Configurando o Stream\n",
        "rdd = ssc.sparkContext.parallelize([0])\n",
        "stream = ssc.queueStream([], default = rdd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIizx_QzoSnz",
        "outputId": "0a6c8039-9f46-4e4f-ac6d-9d20d3b205f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pyspark.streaming.dstream.DStream"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(stream)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMKBsLtloSnz"
      },
      "outputs": [],
      "source": [
        "# Total de tweets por update\n",
        "NUM_TWEETS = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec3sUN2ooSnz"
      },
      "outputs": [],
      "source": [
        "# Essa função conecta ao Twitter e retorna um número específico de Tweets (NUM_TWEETS)\n",
        "def tfunc(t, rdd):\n",
        "  return rdd.flatMap(lambda x: stream_twitter_data())\n",
        "\n",
        "def stream_twitter_data():\n",
        "  response = requests.get(filter_url, auth = auth, stream = True)\n",
        "  print(filter_url, response)\n",
        "  count = 0\n",
        "  for line in response.iter_lines():\n",
        "    try:\n",
        "      if count > NUM_TWEETS:\n",
        "        break\n",
        "      post = json.loads(line.decode('utf-8'))\n",
        "      contents = [post['text']]\n",
        "      count += 1\n",
        "      yield str(contents)\n",
        "    except:\n",
        "      result = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gUHqFuqoSnz"
      },
      "outputs": [],
      "source": [
        "stream = stream.transform(tfunc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyJ-cLrMoSnz"
      },
      "outputs": [],
      "source": [
        "coord_stream = stream.map(lambda line: ast.literal_eval(line))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWEuioBGoSn0"
      },
      "outputs": [],
      "source": [
        "# Essa função classifica os tweets, aplicando as features do modelo criado anteriormente\n",
        "def classifica_tweet(tweet):\n",
        "  sentence = [(tweet, '')]\n",
        "  test_set = sentiment_analyzer.apply_features(sentence)\n",
        "  print(tweet, classifier.classify(test_set[0][0]))\n",
        "  return(tweet, classifier.classify(test_set[0][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZuTpM_1oSn0"
      },
      "outputs": [],
      "source": [
        "# Essa função retorna o texto do Twitter\n",
        "def get_tweet_text(rdd):\n",
        "  for line in rdd:\n",
        "    tweet = line.strip()\n",
        "    translator = str.maketrans({key: None for key in string.punctuation})\n",
        "    tweet = tweet.translate(translator)\n",
        "    tweet = tweet.split(' ')\n",
        "    tweet_lower = []\n",
        "    for word in tweet:\n",
        "      tweet_lower.append(word.lower())\n",
        "    return(classifica_tweet(tweet_lower))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J40aNHpCoSn0"
      },
      "outputs": [],
      "source": [
        "# Cria uma lista vazia para os resultados\n",
        "resultados = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZzzbrMCoSn4"
      },
      "outputs": [],
      "source": [
        "# Essa função salva o resultado dos batches de Tweets junto com o timestamp\n",
        "def output_rdd(rdd):\n",
        "  global resultados\n",
        "  pairs = rdd.map(lambda x: (get_tweet_text(x)[1],1))\n",
        "  counts = pairs.reduceByKey(add)\n",
        "  output = []\n",
        "  for count in counts.collect():\n",
        "    output.append(count)\n",
        "  result = [time.strftime(\"%I:%M:%S\"), output]\n",
        "  resultados.append(result)\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D53r5c8oSn4"
      },
      "outputs": [],
      "source": [
        "# A função foreachRDD() aplica uma função a cada RDD to streaming de dados\n",
        "coord_stream.foreachRDD(lambda t, rdd: output_rdd(rdd))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fXY57l8oSn4"
      },
      "outputs": [],
      "source": [
        "# Start streaming\n",
        "ssc.start()\n",
        "# ssc.awaitTermination()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpFAlbQOoSn4",
        "outputId": "62a01d26-096e-42b3-c276-72219db0aef1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['01:06:09', []]\n",
            "['01:06:37', [('0', 387), ('1', 114)]]\n",
            "['01:07:06', [('1', 97), ('0', 404)]]\n",
            "['01:07:35', [('0', 390), ('1', 111)]]\n",
            "['01:08:05', [('0', 375), ('1', 126)]]\n",
            "['01:08:32', [('0', 402), ('1', 99)]]\n",
            "['01:08:58', [('0', 406), ('1', 95)]]\n"
          ]
        }
      ],
      "source": [
        "cont = True\n",
        "while cont:\n",
        "  if len(resultados) > 5:\n",
        "    cont = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPVgVt_goSn4"
      },
      "outputs": [],
      "source": [
        "# Grava os resultados\n",
        "rdd_save = '/opt/DSA/BigDataAnalytics-Python-Spark/Cap08/r'+time.strftime(\"%I%M%S\")\n",
        "resultados_rdd = sc.parallelize(resultados)\n",
        "resultados_rdd.saveAsTextFile(rdd_save)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6XhZUs3oSn4",
        "outputId": "c88bfe01-8acd-4725-a90e-a7c4bc447cb4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['01:06:09', []],\n",
              " ['01:06:37', [('0', 387), ('1', 114)]],\n",
              " ['01:07:06', [('1', 97), ('0', 404)]],\n",
              " ['01:07:35', [('0', 390), ('1', 111)]],\n",
              " ['01:08:05', [('0', 375), ('1', 126)]],\n",
              " ['01:08:32', [('0', 402), ('1', 99)]],\n",
              " ['01:08:58', [('0', 406), ('1', 95)]]]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Visualiza os resultados\n",
        "resultados_rdd.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XO-bhD-HoSn4"
      },
      "outputs": [],
      "source": [
        "# Finaliza o streaming\n",
        "ssc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt7mJQu7oSn4"
      },
      "source": [
        "# Fim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SazlphhYoSn4"
      },
      "source": [
        "### Obrigado - Data Science Academy - <a href=\"http://facebook.com/dsacademybr\">facebook.com/dsacademybr</a>"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}